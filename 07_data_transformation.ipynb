{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8f02d9-cd79-47c5-a299-7eda1617c5a2",
   "metadata": {},
   "source": [
    "\n",
    "# Data and Image Transformation in Pytorch\n",
    "\n",
    "\n",
    "    Transforms are common image transformations\n",
    "\n",
    "    from torchvision import transforms\n",
    "    import torchvision.transforms.functional as F \n",
    "    \n",
    "    from torchvision.transforms import v2\n",
    "    import torchvision.transforms.v2.functional as F\n",
    "\n",
    "\n",
    "link: https://pytorch.org/vision/stable/transforms.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68319b89-394e-43dd-bdb3-7b2faf3987a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PIL: Python Imaging Library\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms.v2.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0bc25-0679-4c5f-beba-6960066e0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('nature.jpg') \n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236a487-71fd-4ba3-8fd3-05cde742d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "    # p (float) – probability of the image being flipped. Default value is 0.5\n",
    "transform =  transforms.RandomHorizontalFlip(p=0.5)\n",
    "out = transform(image)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cdd9-053d-4863-af23-c1a12a1657e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees : Range of degrees to select from - (min, max)\n",
    "transform =  transforms.RandomRotation(degrees=(-180,180))\n",
    "out = transform(image)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12e845-ccd8-4471-9ac0-2a5289241f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d0a17-faf9-4cbb-9cde-544b65d289d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomCrop(size=(224, 224))\n",
    "out = transform(image)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8cfb1-beb0-4aa4-bfbf-7e8ca94ca404",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Grayscale()\n",
    "out = transform(image)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78596b81-2203-4df5-be9d-d8b6758014b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =  transforms.CenterCrop(size=(200,200))\n",
    "out = transform(image)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =  v2.Resize(size=250) # (h, w)\n",
    "out = transform(image)\n",
    "out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e527ccd-b2b0-4be7-9eb9-6e1e5ffd4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = F.adjust_brightness(image, brightness_factor=3.0) \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54d483-aeee-4ee5-bc35-ed7f1623dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = F.adjust_contrast(image,contrast_factor=4.2) \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958bff2-8ab5-416a-a640-2dbdb8b58888",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = F.adjust_hue(image,hue_factor=-0.3) \n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6aa24e-b7f0-4de8-bc18-c41fd63d928a",
   "metadata": {},
   "source": [
    "# Conversion:\n",
    "- ToPILImage: from tensor or ndarray to image\n",
    "- ToTensor: from numpy.ndarray or PILImage to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53acd5-0515-451f-b66d-4785d2d698f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a PIL Image or numpy.ndarray in the range [0, 255] \n",
    "#                to a torch.FloatTensor in the range [0.0, 1.0]\n",
    "\n",
    "transform =  transforms.ToTensor()\n",
    "tensor_out = transform(image)\n",
    "tensor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e19f0c-a26b-43e6-9c42-5ef1370ab47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a PIL Image or numpy.ndarray in the range [0, 255] \n",
    "#                to a torch.FloatTensor in the range [0.0, 1.0]\n",
    "\n",
    "transform =  transforms.ToPILImage()\n",
    "out = transform(tensor_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41edeb8-0844-4141-810f-699964c74a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a tensor, ndarray, or PIL Image to Image\n",
    "# In the transforms, Image instances are largely interchangeable with pure torch.Tensor\n",
    "\n",
    "transform = v2.ToImage()\n",
    "image_tensor_out = transform(image)\n",
    "image_tensor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b3257-1b7e-495e-a0a0-2c84202b83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =  transforms.ToPILImage()\n",
    "out = transform(image_tensor_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b0831-7ea9-488c-8dad-33758c7d2787",
   "metadata": {},
   "source": [
    "# Compose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916c3be-e8b0-41a8-b646-e625c4b3b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = v2.Compose([\n",
    "     v2.ToImage(), # Convert to tensor, only needed if you had a PIL image\n",
    "     v2.ToDtype(torch.float32, scale=True)\n",
    " ])\n",
    "compose_img = transform(image)\n",
    "compose_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4afbef-0002-479d-9000-0a375588f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20007030-94f5-4691-99da-fc2da2623fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =  transforms.ToPILImage()\n",
    "out = transform(compose_img)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125df705-92c3-4338-82bc-8b3d507f8164",
   "metadata": {},
   "source": [
    "# Normalize\n",
    "\n",
    "### Parameters:\n",
    "- mean (sequence) – Sequence of means for each channel.\n",
    "\n",
    "- std (sequence) – Sequence of standard deviations for each channel.\n",
    "\n",
    "- inplace (bool,optional) – Bool to make this operation in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45a535-7458-4245-8d5f-0ba01012fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToImage(), # Convert to tensor, only needed if you had a PIL image\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "compose_img_2 = transform(image)\n",
    "compose_img_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb8d03-6d90-462c-9aec-75c8220115a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =  transforms.ToPILImage()\n",
    "out = transform(compose_img_2)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d939e90-f069-4c9e-8192-952285576023",
   "metadata": {},
   "source": [
    "# Custom Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23381b-7e03-421e-aee8-796be1ce573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2df7cc-e4bd-4aa0-b86a-a4fc83908cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adding transform to our dataset\n",
    "class WineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, transform=None):\n",
    "        data = np.loadtxt('./wine.csv',delimiter=',',dtype=np.float32,skiprows=1)\n",
    "        self.n_smaples = data.shape[0]\n",
    "        \n",
    "         # note that we do NOT convert to tensor here\n",
    "        self.x = data[:,1:] \n",
    "        self.y = data[:,[0]] \n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_smaples\n",
    "# we have not transformed our x, y data\n",
    "# our class accepts a transform argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89944dbc-1772-4881-800e-376fd213aab3",
   "metadata": {},
   "source": [
    "### The \\__call__ method \n",
    "enables Python programmers to write classes where the instances behave like functions and can be called like a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87949b4-77a1-4cf0-ae96-bcc5b5b3879d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create custom transform for our dataset class\n",
    "\n",
    "class To_Tensor:\n",
    "    def __call__(self,sample):\n",
    "        inputs, targets = sample\n",
    "        \n",
    "        transformed_inputs  = torch.from_numpy(inputs)\n",
    "        transformed_targets = torch.from_numpy(targets)\n",
    "        \n",
    "        return transformed_inputs , transformed_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f0f7a-61e6-4657-84bc-8f7d7d8c101e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = WineDataset()\n",
    "transformed_dataset = WineDataset(transform=To_Tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45797e4-e179-48ab-9b9f-01c47c55fb63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_data = dataset[0]\n",
    "first_data_transformed =  transformed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e1b87c-2440-4059-be82-c538b144df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc41a7-7363-41a1-a144-b7cdbaec23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d7520-7e54-47cb-87b0-7204e8e8d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d935d-f67e-4d2f-8ae9-359d5be2b1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a75ac-f240-4659-82ad-b282c73b6c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd4bba-7cb5-46a0-903e-9f7559d262c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
